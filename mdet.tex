\documentclass[twocolappendix, appendixfloats, numberedappendix, twocolumn, apj]{openjournal}

\usepackage{graphicx}
\usepackage{latexsym,amssymb}
\usepackage{amsmath,morefloats}
\usepackage[backref,breaklinks,colorlinks,citecolor=blue]{hyperref}
\usepackage{natbib,graphicx,amsmath,subfigure,color,xcolor}
\usepackage{verbatim}
\usepackage{threeparttable}
\usepackage{xspace}

\topmargin-1cm

\newcommand{\ess}[1]{\textcolor{red}{[ESS: \bf #1]}\xspace}
\newcommand{\mrb}[1]{\textcolor{purple}{[MRB: \bf #1]}\xspace}

\newcommand{\mdet}{\textsc{metadetection}\xspace}
\newcommand{\mcal}{\textsc{metacalibration}\xspace}

\shorttitle{\mdet with coadding}
\shortauthors{Becker, Sheldon \& Jarvis}

\begin{document}
\title{A Baseline Strategy for Implementing Metadetection Shear Measurements in Real Surveys}

\author{Matthew R. Becker}
\affil{High Energy Physics Division, Argonne National Laboratory, Lemont, IL 60439, USA}
\author{Erin S. Sheldon}
\affil{Brookhaven National Laboratory, Bldg 510, Upton, New York 11973, USA}
\author{Michael Jarvis}
\affil{Department of Physics and Astronomy, University of Pennsylvania, Philadelphia, PA 19104, USA}


\begin{abstract}
\mdet is a promising technique for making weak gravitational lensing measurements in the presence of
object detection and blending. Currently published tests of this algorithm were only done in
highly idealized situations, neglecting the effects of pixel-level artifact masking, image edges,
and image coaddition through world coordinate system transforms. In this work, we develop algorithms
to address each of these effects and demonstrate that \mdet weak lensing shear measurements are
unbiased to better than 0.1\% even in the presence of these effects. Looking forward, this work provides
a baseline set of algorithms that can be implemented in the analysis pipelines of real surveys.
Future challenges remain, especially concerning image background subtraction, the handling of bright stars, and
pixel noise correlations introduced by sensor effects. Work on these effects is ongoing. Further
ongoing work in the Dark Energy Survey will provide the first \mdet measurements for precision
cosmology and potentially bring yet unknown issues to light. The ultimate goal of analyzing the
full data sets from the next generation of surveys, such as the Legacy Survey of Space and Time
from the Vera C. Rubin Observatory, will require yet more detailed testing in even more detailed
and realistic simulations.
\end{abstract}


\section{Introduction}\label{sec:intro}

\section{Weak Lensing \& \mdet}\label{sec:background}

\section{Simulations and Methodology}\label{sec:sims}

\subsection{Image Interpolation and Coadding Procedures}

\mdet's defining feature is the re-detection of sources in each of the \mcal
sheared images. This procedure constrains possible algorithms for implementing
\mdet in real surveys in several ways.

\begin{itemize}
  \item The regions over which we apply \mdet
  must be big enough that detection algorithms run correctly.
  \item As \mcal requires deconvolving the PSF, the regions we use must not have
  single-epoch image edges and or missing pixels for some of the input images. These features
  would result in PSF discontinuities.
  \item The PSF of the resulting coadd must be computable from the input image PSFs.
  \item A sample of the noise correlation field must be computable from the input images.
  This noise sample is used to cancel the effects of the sheared noise generated
  by the \mcal process.
  \item Variable PSF deconvolutions, while possible, are numerically expensive, so the input
  region must be small enough that we can safely neglect any residual PSF variation in the coadd image.
\end{itemize}

While the constraint that we need to apply detection algorithms to the coadd argues
for larger coadd regions, the constraint that we need an exact, constant, computable
PSF for the resulting coadd image argues for smaller coadd regions. This last constraint is
particularly an issue for modern surveys where the gaps between CCDs on the focal plane
combined with semi-structured tilling make finding a large, CCD edge-free region difficult
if not impossible to find. Ultimately, the size of the coadd region will be compromise
between these different things and will greatly depend on the details of a given survey.
With these considerations in mind, we target our tests at coadd regions of about 1 arcminute
on a side, which appear to be close to optimal for an LSST-like survey \citep{ArmstrongCoadd}.

Given the target size of the coadd region, one must choose a suitable projection
and pixel scale for the coadd coordinate system. With this choice, the coadding process
works as follows (see \ref{fig:flowchart}):

\begin{enumerate}
\item Find every single-epoch that completely contains within its boundaries the entire coadd region.
\item Find the pixel center closest to the center of the coadd region, defined here as the coadd center.
\item For each of these single-epoch images:
\begin{enumerate}
  \item Generate a noise realization with the same pixel noise correlations.
  \item Interpolate all pixels with missing data in the image.
  \item Apply the same interpolation algorithm to the same pixels in the generated noise image.
  \item Compute the location of the coadd center in the single-epoch image coordinates.
  \item Generate an image of the PSF at the location of the coadd center in the single-epoch image coordinates.
  \item Construct a masked fraction map for the single-epoch image which marks the fraction of each pixel
  which was interpolated (i.e., either 0 or 1 in this case).
  \item Resample the image, the noise image, the PSF image, and the masked fraction image to the coadd coordinate system.
\end{enumerate}
\item Using the resampled single-epoch data products from the last step, form a (possibly weighted) coadd image
by summing them together with an appropriately normalized set of weights.
\end{enumerate}
For image interpolation, we use a \mrb{describe} in the \texttt{scipy} package \citep{scipy}. For image resampling,
we use a Lanczos-3 interpolation as a compromise between optimally resampling the image and artificially
smoothing the image more than needed. In real survey processing, one usually keeps bit masks indicating
processing steps done to each pixel. Those too can be propagated through these steps. One very important
feature of this algorithm is where we draw the PSF. By drawing the PSF at the single-epoch location for each
image that corresponds to the same location in the coadd coordinate system, we properly track how the resampling
process slightly broadens the PSF in the final coadd. Further, by putting pure noise images through the same
coadding process, we automatically generate a noise image with the proper noise correlations in the final coadd.
This noise image is needed by \mcal, as described above. Finally, we show below that by measuring the weighted
amplitude of the coadd masked fraction image at each detection center, we can incorporate a masked fraction cut
into \mdet that helps to reject objects with excessively interpolated flux.



\subsection{Simulations and Pixel-level Artifact Generation}

\subsection{Simulation Analysis}

\section{Results}\label{sec:results}

\section{Summary}\label{sec:conc}

\section*{Acknowledgments}

ESS is supported by DOE grant DE-AC02-98CH10886, and MRB is supported by DOE
grant DE-AC02-06CH11357.  We gratefully acknowledge the computing resources
provided on Bebop, a high-performance computing cluster operated by the
Laboratory Computing Resource Center at Argonne National Laboratory, and the
RHIC Atlas Computing Facility, operated by Brookhaven National Laboratory.
This work also used resources made available on the Phoenix cluster, a joint
data-intensive computing project between the High Energy Physics Division and
the Computing, Environment, and Life Sciences (CELS) Directorate at Argonne
National Laboratory.

\bibliographystyle{aasjournal}
\bibliography{references}

% \appendix

\end{document}
